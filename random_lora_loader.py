"""
RandomLoRALoader - ComfyUI Custom Node V1

ã€ä»•æ§˜æ¦‚è¦ã€‘
æŒ‡å®šã—ãŸ3ã¤ã®ãƒ•ã‚©ãƒ«ãƒ€å†…ã®LoRAã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠã—ã€MODELã¨CLIPã«é©ç”¨ã™ã‚‹ãƒãƒ¼ãƒ‰

ã€å…¥åŠ›ã€‘
- MODEL: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
- CLIP: ãƒ™ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼

ã€å‡ºåŠ›ã€‘
- MODEL: LoRAé©ç”¨å¾Œã®ãƒ¢ãƒ‡ãƒ«
- CLIP: LoRAé©ç”¨å¾Œã®CLIP
- positive_text: é©ç”¨ã—ãŸLoRAæƒ…å ±ã¨ãƒˆãƒªã‚¬ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆpositiveç¢ºèªç”¨ï¼‰
  å½¢å¼: è¿½åŠ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆ1è¡Œç›®ï¼‰+ å„LoRAæƒ…å ±ï¼ˆæ”¹è¡ŒåŒºåˆ‡ã‚Šï¼‰
- negative_text: negativeãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆjson_sample_prompté¸æŠæ™‚ã®ã¿ï¼‰
- CONDITIONING (positive): è¿½åŠ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ+ãƒˆãƒªã‚¬ãƒ¼ãƒ¯ãƒ¼ãƒ‰é©ç”¨å¾Œã®conditioning
- CONDITIONING (negative): ä½œä¾‹å–å¾—æ™‚ã®negativeãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

ã€è¨­å®šé …ç›®ã€‘ï¼ˆè¡¨ç¤ºé †ï¼‰
1. token_normalization: none/mean/length/length+meanï¼ˆå…±é€šï¼‰
2. weight_interpretation: comfy/A1111/compel/comfy++/down_weightï¼ˆå…±é€šï¼‰
3. additional_prompt: è¿½åŠ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆãƒˆãƒªã‚¬ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨çµåˆã—ã¦CONDITIONINGã«é©ç”¨ï¼‰ï¼ˆå…±é€šï¼‰
4-8. ã‚°ãƒ«ãƒ¼ãƒ—1è¨­å®šï¼ˆä¾‹ï¼šstyleç”¨ï¼‰
   - lora_folder_path_1: LoRAãƒ•ã‚©ãƒ«ãƒ€ã®çµ¶å¯¾ãƒ‘ã‚¹
   - include_subfolders_1: ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã‚’å«ã‚ã‚‹ã‹
   - model_strength_1: MODELé©ç”¨å¼·åº¦
   - clip_strength_1: CLIPé©ç”¨å¼·åº¦
   - num_loras_1: é¸æŠã™ã‚‹LoRAå€‹æ•°
9-13. ã‚°ãƒ«ãƒ¼ãƒ—2è¨­å®šï¼ˆä¾‹ï¼šcharacterç”¨ï¼‰
   - lora_folder_path_2: LoRAãƒ•ã‚©ãƒ«ãƒ€ã®çµ¶å¯¾ãƒ‘ã‚¹
   - include_subfolders_2: ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã‚’å«ã‚ã‚‹ã‹
   - model_strength_2: MODELé©ç”¨å¼·åº¦
   - clip_strength_2: CLIPé©ç”¨å¼·åº¦
   - num_loras_2: é¸æŠã™ã‚‹LoRAå€‹æ•°
14-18. ã‚°ãƒ«ãƒ¼ãƒ—3è¨­å®šï¼ˆä¾‹ï¼šconceptç”¨ï¼‰
   - lora_folder_path_3: LoRAãƒ•ã‚©ãƒ«ãƒ€ã®çµ¶å¯¾ãƒ‘ã‚¹
   - include_subfolders_3: ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã‚’å«ã‚ã‚‹ã‹
   - model_strength_3: MODELé©ç”¨å¼·åº¦
   - clip_strength_3: CLIPé©ç”¨å¼·åº¦
   - num_loras_3: é¸æŠã™ã‚‹LoRAå€‹æ•°
19. trigger_word_source: json_combined/json_random/json_sample_promptï¼ˆå…±é€šï¼‰
20. seed: ãƒ©ãƒ³ãƒ€ãƒ é¸æŠã®ã‚·ãƒ¼ãƒ‰å€¤ï¼ˆå…±é€šã€ComfyUIæ¨™æº–ã®control_before_generateã§åˆ¶å¾¡ï¼‰

ã€ãã®ä»–ä»•æ§˜ã€‘
- å¤–éƒ¨JSONãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ{LoRAãƒ•ã‚¡ã‚¤ãƒ«å}.metadata.jsonï¼‰ã‹ã‚‰ãƒˆãƒªã‚¬ãƒ¼ãƒ¯ãƒ¼ãƒ‰/ä½œä¾‹ã‚’å–å¾—
- JSONèª­ã¿å–ã‚Šå„ªå…ˆé †ä½: trainedWords / images[].meta.prompt
- åŒã˜LoRAã®é‡è¤‡é¸æŠãªã—ï¼ˆä¸è¶³æ™‚ã¯å†é¸æŠã§åŸ‹ã‚ã‚‹ï¼‰
- json_sample_prompté¸æŠæ™‚ã¯positive/negativeã‚’åˆ†é›¢
- ä½œä¾‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã®LoRAè¨˜è¿°ï¼ˆ<lora:xxx:x.x>ï¼‰ã¯å‰Šé™¤
- LoRAé©ç”¨ã¯positiveã®ã¿
- ç©ºã®ãƒ•ã‚©ãƒ«ãƒ€æŒ‡å®šã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚¨ãƒ©ãƒ¼ãªã—ï¼‰
- å…¨ã‚°ãƒ«ãƒ¼ãƒ—ç©ºã§ã‚‚ã‚¨ãƒ©ãƒ¼ãªã—ï¼ˆç©ºãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ï¼‰
"""

import os
import json
import random
import glob
import re
from pathlib import Path
import folder_paths
import comfy.sd
import comfy.utils

class RandomLoRALoader:
    """ãƒ©ãƒ³ãƒ€ãƒ LoRAé¸æŠãƒ»é©ç”¨ãƒãƒ¼ãƒ‰ï¼ˆ3ã‚°ãƒ«ãƒ¼ãƒ—å¯¾å¿œï¼‰"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("MODEL",),
                "clip": ("CLIP",),
                "token_normalization": (
                    ["none", "mean", "length", "length+mean"],
                    {
                        "default": "none"
                    }
                ),
                "weight_interpretation": (
                    ["comfy", "A1111", "compel", "comfy++", "down_weight"],
                    {
                        "default": "A1111"
                    }
                ),
                "additional_prompt": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": "Additional prompt (e.g., 1girl, beautiful)"
                }),
                # ã‚°ãƒ«ãƒ¼ãƒ—1
                "lora_folder_path_1": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "placeholder": "Group 1 LoRA folder path (e.g., style)"
                }),
                "include_subfolders_1": ("BOOLEAN", {
                    "default": True
                }),
                "model_strength_1": ("STRING", {
                    "default": "1.0",
                    "multiline": False,
                    "placeholder": "e.g., 1.0 or 0.4-0.8"
                }),
                "clip_strength_1": ("STRING", {
                    "default": "1.0",
                    "multiline": False,
                    "placeholder": "e.g., 1.0 or 0.4-0.8"
                }),
                "num_loras_1": ("INT", {
                    "default": 1,
                    "min": 0,
                    "max": 20,
                    "step": 1,
                    "display": "number"
                }),
                # ã‚°ãƒ«ãƒ¼ãƒ—2
                "lora_folder_path_2": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "placeholder": "Group 2 LoRA folder path (e.g., character)"
                }),
                "include_subfolders_2": ("BOOLEAN", {
                    "default": True
                }),
                "model_strength_2": ("STRING", {
                    "default": "1.0",
                    "multiline": False,
                    "placeholder": "e.g., 1.0 or 0.4-0.8"
                }),
                "clip_strength_2": ("STRING", {
                    "default": "1.0",
                    "multiline": False,
                    "placeholder": "e.g., 1.0 or 0.4-0.8"
                }),
                "num_loras_2": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 20,
                    "step": 1,
                    "display": "number"
                }),
                # ã‚°ãƒ«ãƒ¼ãƒ—3
                "lora_folder_path_3": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "placeholder": "Group 3 LoRA folder path (e.g., concept)"
                }),
                "include_subfolders_3": ("BOOLEAN", {
                    "default": True
                }),
                "model_strength_3": ("STRING", {
                    "default": "1.0",
                    "multiline": False,
                    "placeholder": "e.g., 1.0 or 0.4-0.8"
                }),
                "clip_strength_3": ("STRING", {
                    "default": "1.0",
                    "multiline": False,
                    "placeholder": "e.g., 1.0 or 0.4-0.8"
                }),
                "num_loras_3": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 20,
                    "step": 1,
                    "display": "number"
                }),
                # å…±é€šè¨­å®š
                "trigger_word_source": (
                    ["json_combined", "json_random", "json_sample_prompt"],
                    {
                        "default": "json_combined"
                    }
                ),
                "seed": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 0xffffffffffffffff,
                    "step": 1,
                    "display": "number"
                }),
            }
        }
    
    RETURN_TYPES = ("MODEL", "CLIP", "STRING", "STRING", "CONDITIONING", "CONDITIONING")
    RETURN_NAMES = ("MODEL", "CLIP", "positive_text", "negative_text", "positive", "negative")
    FUNCTION = "load_random_loras"
    CATEGORY = "loaders"
    
    def _find_lora_files(self, folder_path, include_subfolders):
        """
        æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€å†…ã®LoRAãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.safetensorsï¼‰ã‚’æ¤œç´¢
        
        Args:
            folder_path: æ¤œç´¢å¯¾è±¡ãƒ•ã‚©ãƒ«ãƒ€ã®çµ¶å¯¾ãƒ‘ã‚¹
            include_subfolders: ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã‚’å«ã‚ã‚‹ã‹
        
        Returns:
            list: LoRAãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        """
        if not os.path.exists(folder_path):
            print(f"[RandomLoRALoader] ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {folder_path}")
            return []
        
        pattern = "**/*.safetensors" if include_subfolders else "*.safetensors"
        lora_files = glob.glob(os.path.join(folder_path, pattern), recursive=include_subfolders)
        
        print(f"[RandomLoRALoader] æ¤œå‡ºã•ã‚ŒãŸLoRAæ•°: {len(lora_files)}")
        return lora_files
    
    def _select_random_loras(self, lora_files, num_loras, seed):
        """
        LoRAãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠï¼ˆé‡è¤‡ãªã—ã€ä¸è¶³æ™‚ã¯å†é¸æŠï¼‰
        
        Args:
            lora_files: LoRAãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            num_loras: é¸æŠã™ã‚‹å€‹æ•°
            seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
        
        Returns:
            list: é¸æŠã•ã‚ŒãŸLoRAãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        """
        if not lora_files:
            return []
        
        random.seed(seed)
        
        # é‡è¤‡ãªã—ã§é¸æŠã§ãã‚‹æœ€å¤§æ•°
        available_count = len(lora_files)
        
        if num_loras <= available_count:
            # ååˆ†ãªæ•°ãŒã‚ã‚‹å ´åˆã¯é‡è¤‡ãªã—ã§é¸æŠ
            return random.sample(lora_files, num_loras)
        else:
            # ä¸è¶³ã™ã‚‹å ´åˆã¯å…¨ã¦é¸æŠå¾Œã€å†é¸æŠã§åŸ‹ã‚ã‚‹
            selected = lora_files.copy()
            remaining = num_loras - available_count
            
            # ä¸è¶³åˆ†ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è¿½åŠ 
            for _ in range(remaining):
                selected.append(random.choice(lora_files))
            
            # æœ€çµ‚çš„ã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«
            random.shuffle(selected)
            return selected
    
    def _parse_strength(self, strength_str):
        """
        å¼·åº¦æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦å€¤ã‚’è¿”ã™
        
        å¯¾å¿œå½¢å¼:
        - "1.0" â†’ 1.0ï¼ˆãã®ã¾ã¾ï¼‰
        - "0.55" â†’ 0.55ï¼ˆãã®ã¾ã¾ï¼‰
        - "0.4-0.8" â†’ 0.4, 0.5, 0.6, 0.7, 0.8ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ï¼ˆ0.1åˆ»ã¿ï¼‰
        - "0.44-0.82" â†’ 0.4, 0.5, 0.6, 0.7, 0.8ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ï¼ˆç¯„å›²ã‚’1æ¡ã«ä¸¸ã‚ã‚‹ï¼‰
        
        Args:
            strength_str: å¼·åº¦æ–‡å­—åˆ—
        
        Returns:
            float: å®Ÿéš›ã«ä½¿ç”¨ã™ã‚‹å¼·åº¦å€¤
        """
        strength_str = str(strength_str).strip()
        
        # ãƒã‚¤ãƒ•ãƒ³ãŒã‚ã‚Œã°ãƒ©ãƒ³ãƒ€ãƒ ç¯„å›²æŒ‡å®š
        if '-' in strength_str:
            try:
                parts = strength_str.split('-')
                if len(parts) != 2:
                    raise ValueError("ç¯„å›²æŒ‡å®šã¯ 'min-max' å½¢å¼ã§å…¥åŠ›ã—ã¦ãã ã•ã„")
                
                # ç¯„å›²ã®ä¸Šé™ä¸‹é™ã‚’1æ¡ã«ä¸¸ã‚ã‚‹
                min_val = round(float(parts[0].strip()), 1)
                max_val = round(float(parts[1].strip()), 1)
                
                # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                if min_val < -10.0 or max_val > 10.0:
                    raise ValueError("å¼·åº¦ã¯ -10.0 ã€œ 10.0 ã®ç¯„å›²ã§æŒ‡å®šã—ã¦ãã ã•ã„")
                if min_val > max_val:
                    raise ValueError("æœ€å°å€¤ã¯æœ€å¤§å€¤ã‚ˆã‚Šå°ã•ãã—ã¦ãã ã•ã„")
                
                # 0.1åˆ»ã¿ã®å€¤ã®ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
                values = []
                current = min_val
                while current <= max_val + 0.01:  # æµ®å‹•å°æ•°ç‚¹èª¤å·®å¯¾ç­–
                    values.append(round(current, 1))
                    current += 0.1
                
                # å¿µã®ãŸã‚ç©ºãƒªã‚¹ãƒˆãƒã‚§ãƒƒã‚¯
                if not values:
                    print(f"[RandomLoRALoader] âŒ ç¯„å›²æŒ‡å®šã‚¨ãƒ©ãƒ¼ã€1.0ã‚’ä½¿ç”¨")
                    return 1.0
                
                # ãƒ©ãƒ³ãƒ€ãƒ ã«1ã¤é¸æŠ
                selected = random.choice(values)
                print(f"[RandomLoRALoader] å¼·åº¦ç¯„å›² {min_val}-{max_val} ã‹ã‚‰ {selected} ã‚’é¸æŠ")
                return selected
                
            except ValueError as e:
                print(f"[RandomLoRALoader] âŒ å¼·åº¦ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                print(f"[RandomLoRALoader] ğŸ’¡ ä½¿ç”¨ä¾‹: '1.0' ã¾ãŸã¯ '0.4-0.8'")
                return 1.0
            except Exception as e:
                print(f"[RandomLoRALoader] âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
                return 1.0
        else:
            # é€šå¸¸ã®æ•°å€¤ï¼ˆãã®ã¾ã¾ä½¿ç”¨ï¼‰
            try:
                value = float(strength_str)
                if value < -10.0 or value > 10.0:
                    print(f"[RandomLoRALoader] âŒ å¼·åº¦ {value} ã¯ç¯„å›²å¤–ã€1.0ã‚’ä½¿ç”¨")
                    return 1.0
                return value
            except:
                print(f"[RandomLoRALoader] âŒ å¼·åº¦ '{strength_str}' ã‚’è§£æã§ãã¾ã›ã‚“ã€1.0ã‚’ä½¿ç”¨")
                print(f"[RandomLoRALoader] ğŸ’¡ ä½¿ç”¨ä¾‹: '1.0' ã¾ãŸã¯ '0.4-0.8'")
                return 1.0
    
    def _load_json_metadata(self, lora_path):
        """
        å¤–éƒ¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
        
        Args:
            lora_path: LoRAãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (.safetensors)
        
        Returns:
            dict: JSONãƒ‡ãƒ¼ã‚¿ï¼ˆèª­ã¿è¾¼ã¿å¤±æ•—æ™‚ã¯Noneï¼‰
        """
        # .safetensorsã‚’é™¤ã„ãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—
        base_name = os.path.splitext(lora_path)[0]
        json_path = f"{base_name}.metadata.json"
        
        if not os.path.exists(json_path):
            print(f"[RandomLoRALoader] JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {json_path}")
            return None
        
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"[RandomLoRALoader] JSONèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({json_path}): {e}")
            return None
    
    def _get_trigger_words_combined(self, lora_path):
        """
        JSONã‹ã‚‰å…¨ãƒˆãƒªã‚¬ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’çµåˆã—ã¦å–å¾—ï¼ˆé‡è¤‡é™¤å»ï¼‰
        
        Args:
            lora_path: LoRAãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        
        Returns:
            str: å…¨ãƒˆãƒªã‚¬ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’çµåˆã—ãŸæ–‡å­—åˆ—
        """
        json_data = self._load_json_metadata(lora_path)
        if not json_data:
            return ""
        
        # civitai.trainedWordsã‚’å–å¾—
        trained_words = json_data.get("civitai", {}).get("trainedWords", [])
        if not trained_words:
            print(f"[RandomLoRALoader] trainedWordsãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {lora_path}")
            return ""
        
        # å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’çµåˆã—ã¦é‡è¤‡é™¤å»
        all_tags = []
        for pattern in trained_words:
            tags = [tag.strip() for tag in pattern.split(',')]
            all_tags.extend(tags)
        
        # é‡è¤‡é™¤å»ï¼ˆé †åºã‚’ä¿æŒï¼‰
        unique_tags = []
        seen = set()
        for tag in all_tags:
            if tag and tag.lower() not in seen:
                unique_tags.append(tag)
                seen.add(tag.lower())
        
        return ", ".join(unique_tags)
    
    def _get_trigger_words_random(self, lora_path):
        """
        JSONã‹ã‚‰ãƒˆãƒªã‚¬ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«1ã¤é¸æŠ
        
        Args:
            lora_path: LoRAãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        
        Returns:
            str: ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠã•ã‚ŒãŸãƒˆãƒªã‚¬ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        """
        json_data = self._load_json_metadata(lora_path)
        if not json_data:
            return ""
        
        # civitai.trainedWordsã‚’å–å¾—
        trained_words = json_data.get("civitai", {}).get("trainedWords", [])
        if not trained_words:
            print(f"[RandomLoRALoader] trainedWordsãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {lora_path}")
            return ""
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã«1ã¤é¸æŠ
        selected_pattern = random.choice(trained_words)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³å†…ã§é‡è¤‡é™¤å»
        tags = [tag.strip() for tag in selected_pattern.split(',')]
        unique_tags = []
        seen = set()
        for tag in tags:
            if tag and tag.lower() not in seen:
                unique_tags.append(tag)
                seen.add(tag.lower())
        
        return ", ".join(unique_tags)
    
    def _get_sample_prompt_from_json(self, lora_path):
        """
        JSONã‹ã‚‰ä½œä¾‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«1ã¤å–å¾—
        ä½œä¾‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã®LoRAè¨˜è¿°ã‚’å‰Šé™¤
        
        Args:
            lora_path: LoRAãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        
        Returns:
            tuple: (positive_prompt, negative_prompt)
        """
        json_data = self._load_json_metadata(lora_path)
        if not json_data:
            return "", ""
        
        # civitai.imagesã‚’å–å¾—
        images = json_data.get("civitai", {}).get("images", [])
        if not images:
            print(f"[RandomLoRALoader] imagesãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {lora_path}")
            return "", ""
        
        # metaã‚’æŒã¤ç”»åƒã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆmetaãŒNoneã§ãªã„ã“ã¨ã‚‚ç¢ºèªï¼‰
        valid_images = [img for img in images if "meta" in img and img["meta"] is not None]
        if not valid_images:
            print(f"[RandomLoRALoader] metaã‚’æŒã¤ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {lora_path}")
            return "", ""
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã«1ã¤é¸æŠ
        selected_image = random.choice(valid_images)
        meta = selected_image.get("meta", {})
        
        positive = meta.get("prompt", "")
        negative = meta.get("negativePrompt", "")
        
        # LoRAè¨˜è¿°ã‚’å‰Šé™¤ï¼ˆ<lora:xxx:x.x>ã¾ãŸã¯<lora:xxx:x.x:x.x>å½¢å¼ï¼‰
        lora_pattern = r'<lora:[^>]+>'
        positive = re.sub(lora_pattern, '', positive)
        negative = re.sub(lora_pattern, '', negative)
        
        # è¤‡æ•°ã®ç©ºç™½ã‚„ã‚«ãƒ³ãƒã‚’æ•´ç†
        positive = re.sub(r'\s*,\s*,+\s*', ', ', positive)  # é€£ç¶šã‚«ãƒ³ãƒã‚’1ã¤ã«
        positive = re.sub(r'^\s*,\s*|\s*,\s*$', '', positive)  # å…ˆé ­æœ«å°¾ã®ã‚«ãƒ³ãƒå‰Šé™¤
        positive = re.sub(r'\s+', ' ', positive).strip()  # è¤‡æ•°ç©ºç™½ã‚’1ã¤ã«
        
        negative = re.sub(r'\s*,\s*,+\s*', ', ', negative)
        negative = re.sub(r'^\s*,\s*|\s*,\s*$', '', negative)
        negative = re.sub(r'\s+', ' ', negative).strip()
        
        return positive, negative
    
    def _load_lora(self, model, clip, lora_path, model_strength, clip_strength):
        """
        LoRAã‚’MODELã¨CLIPã«é©ç”¨
        
        Args:
            model: å…¥åŠ›MODEL
            clip: å…¥åŠ›CLIP
            lora_path: LoRAãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            model_strength: MODELé©ç”¨å¼·åº¦
            clip_strength: CLIPé©ç”¨å¼·åº¦
        
        Returns:
            tuple: (é©ç”¨å¾ŒMODEL, é©ç”¨å¾ŒCLIP)
        """
        try:
            # LoRAèª­ã¿è¾¼ã¿æ™‚ã®è­¦å‘Šã‚’å®Œå…¨æŠ‘åˆ¶
            import logging
            import warnings
            import sys
            import os
            import builtins
            
            # Pythonã®è­¦å‘Šã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore")
                
                # å…¨ãƒ­ã‚¬ãƒ¼ã®ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’ä¸€æ™‚çš„ã«ä¸Šã’ã‚‹
                loggers_to_suppress = [
                    logging.getLogger("comfy"),
                    logging.getLogger("comfy.sd"),
                    logging.getLogger("comfy.utils"),
                    logging.getLogger(),  # rootãƒ­ã‚¬ãƒ¼
                ]
                original_levels = {}
                for logger in loggers_to_suppress:
                    original_levels[logger] = logger.level
                    logger.setLevel(logging.CRITICAL)
                
                # æ¨™æº–å‡ºåŠ›ãƒ»æ¨™æº–ã‚¨ãƒ©ãƒ¼å‡ºåŠ›ã‚’ä¸¡æ–¹ã¨ã‚‚ä¸€æ™‚çš„ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ
                old_stdout = sys.stdout
                old_stderr = sys.stderr
                devnull = open(os.devnull, 'w')
                sys.stdout = devnull
                sys.stderr = devnull
                
                # builtins.printã‚‚ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ï¼ˆæœ€çµ‚æ‰‹æ®µï¼‰
                original_print = builtins.print
                def silent_print(*args, **kwargs):
                    # "lora key not loaded"ã‚’å«ã‚€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã ã‘æŠ‘åˆ¶
                    message = ' '.join(str(arg) for arg in args)
                    if 'lora key not loaded' not in message.lower():
                        original_print(*args, **kwargs, file=old_stdout)
                builtins.print = silent_print
                
                try:
                    lora = comfy.utils.load_torch_file(lora_path, safe_load=True)
                    
                    # MODELã«LoRAé©ç”¨
                    model_lora, _ = comfy.sd.load_lora_for_models(
                        model, None, lora, model_strength, 0
                    )
                    
                    # CLIPã«LoRAé©ç”¨
                    _, clip_lora = comfy.sd.load_lora_for_models(
                        None, clip, lora, 0, clip_strength
                    )
                    
                    return model_lora, clip_lora
                finally:
                    # ã™ã¹ã¦ã‚’å…ƒã«æˆ»ã™
                    builtins.print = original_print
                    sys.stdout = old_stdout
                    sys.stderr = old_stderr
                    devnull.close()
                    
                    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’å…ƒã«æˆ»ã™
                    for logger, level in original_levels.items():
                        logger.setLevel(level)
        
        except Exception as e:
            print(f"[RandomLoRALoader] LoRAèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({lora_path}): {e}")
            return model, clip
    
    def _remove_lora_syntax(self, text):
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰<lora:xxx:0.8>å½¢å¼ã®æ§‹æ–‡ã‚’å‰Šé™¤
        
        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            LoRAæ§‹æ–‡ã‚’å‰Šé™¤ã—ãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not text:
            return text
        
        # <lora:filename:strength> ã¾ãŸã¯ <lora:filename:model_str:clip_str> ã‚’å‰Šé™¤
        text = re.sub(r'<lora:[^>]+>', '', text)
        
        # é€£ç¶šã‚«ãƒ³ãƒã‚’æ•´ç†
        text = re.sub(r',\s*,', ',', text)
        
        # è¡Œé ­ãƒ»è¡Œæœ«ã®ç©ºç™½ã¨ã‚«ãƒ³ãƒã‚’å‰Šé™¤
        text = text.strip().strip(',').strip()
        
        return text
    
    def _encode_prompt(self, clip, text, token_normalization, weight_interpretation):
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’CLIPã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦CONDITIONINGã‚’ç”Ÿæˆ
        
        Args:
            clip: CLIP
            text: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
            token_normalization: none/mean/length/length+mean
            weight_interpretation: comfy/A1111/compel/comfy++/down_weight
        
        Returns:
            CONDITIONING
        """
        # CLIPã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—
        # Note: ComfyUIã®å®Ÿè£…ã«ä¾å­˜ã™ã‚‹ãŸã‚ã€å®Ÿéš›ã®APIã«åˆã‚ã›ã¦èª¿æ•´ãŒå¿…è¦
        try:
            # token_normalizationã®è¨­å®šã‚’åæ˜ 
            tokens = clip.tokenize(text)
            
            # weight_interpretationã«å¿œã˜ãŸå‡¦ç†
            # ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã¯ComfyUIã®å†…éƒ¨APIã«ä¾å­˜ï¼‰
            
            cond, pooled = clip.encode_from_tokens(tokens, return_pooled=True)
            
            return [[cond, {"pooled_output": pooled}]]
        
        except Exception as e:
            print(f"[RandomLoRALoader] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç©ºã®CONDITIONING
            return [[clip.encode(""), {}]]
    
    def load_random_loras(
        self,
        model,
        clip,
        token_normalization,
        weight_interpretation,
        additional_prompt,
        # ã‚°ãƒ«ãƒ¼ãƒ—1
        lora_folder_path_1,
        include_subfolders_1,
        model_strength_1,
        clip_strength_1,
        num_loras_1,
        # ã‚°ãƒ«ãƒ¼ãƒ—2
        lora_folder_path_2,
        include_subfolders_2,
        model_strength_2,
        clip_strength_2,
        num_loras_2,
        # ã‚°ãƒ«ãƒ¼ãƒ—3
        lora_folder_path_3,
        include_subfolders_3,
        model_strength_3,
        clip_strength_3,
        num_loras_3,
        # å…±é€š
        trigger_word_source,
        seed
    ):
        """
        ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼šãƒ©ãƒ³ãƒ€ãƒ LoRAé¸æŠãƒ»é©ç”¨ï¼ˆ3ã‚°ãƒ«ãƒ¼ãƒ—å¯¾å¿œï¼‰
        
        Returns:
            tuple: (MODEL, CLIP, positive_text_output, negative_text_output, positive_conditioning, negative_conditioning)
        """
        # seedã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼ˆComfyUIã®control_before_generateã§åˆ¶å¾¡ã•ã‚Œã‚‹ï¼‰
        print(f"[RandomLoRALoader] ä½¿ç”¨seed: {seed}")
        
        # å…¨ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ç”¨ã®ãƒªã‚¹ãƒˆ
        all_text_parts = []
        
        # å…¨positive/negativeãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è“„ç©
        all_positive_parts = []
        all_negative_parts = []
        
        # 3ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å‡¦ç†
        groups = [
            (lora_folder_path_1, include_subfolders_1, model_strength_1, clip_strength_1, num_loras_1, "Group 1"),
            (lora_folder_path_2, include_subfolders_2, model_strength_2, clip_strength_2, num_loras_2, "Group 2"),
            (lora_folder_path_3, include_subfolders_3, model_strength_3, clip_strength_3, num_loras_3, "Group 3"),
        ]
        
        for folder_path, include_subs, model_str, clip_str, num, group_name in groups:
            # ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ãŒç©ºã€ã¾ãŸã¯num_lorasãŒ0ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if not folder_path.strip() or num == 0:
                print(f"[RandomLoRALoader] {group_name}: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ•ã‚©ãƒ«ãƒ€æœªæŒ‡å®šã¾ãŸã¯num=0ï¼‰")
                continue
            
            # LoRAãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            lora_files = self._find_lora_files(folder_path, include_subs)
            
            if not lora_files:
                print(f"[RandomLoRALoader] {group_name}: LoRAãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                continue
            
            # ãƒ©ãƒ³ãƒ€ãƒ ã«LoRAã‚’é¸æŠï¼ˆå…¨ã‚°ãƒ«ãƒ¼ãƒ—å…±é€šã®seedã‚’ä½¿ç”¨ï¼‰
            selected_loras = self._select_random_loras(lora_files, num, seed)
            
            print(f"[RandomLoRALoader] {group_name}: {len(selected_loras)}å€‹ã®LoRAã‚’é¸æŠ")
            
            # å„LoRAã‚’é †æ¬¡é©ç”¨
            for lora_path in selected_loras:
                lora_name = os.path.splitext(os.path.basename(lora_path))[0]
                
                # å¼·åº¦æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ç¯„å›²å¯¾å¿œï¼‰
                actual_model_str = self._parse_strength(model_str)
                actual_clip_str = self._parse_strength(clip_str)
                
                # LoRAé©ç”¨ãƒ­ã‚°ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã¨å¼·åº¦ã‚’è¡¨ç¤ºï¼‰
                print(f"[RandomLoRALoader]   â†’ {lora_name} (MODEL:{actual_model_str}, CLIP:{actual_clip_str})")
                
                # LoRAã‚’MODELã¨CLIPã«é©ç”¨
                model, clip = self._load_lora(model, clip, lora_path, actual_model_str, actual_clip_str)
                
                # ãƒˆãƒªã‚¬ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—ï¼ˆã‚½ãƒ¼ã‚¹ã«å¿œã˜ã¦å‡¦ç†ã‚’åˆ†å²ï¼‰
                if trigger_word_source == "json_combined":
                    trigger_words = self._get_trigger_words_combined(lora_path)
                    all_positive_parts.append(trigger_words)
                    trigger_display = trigger_words
                
                elif trigger_word_source == "json_random":
                    trigger_words = self._get_trigger_words_random(lora_path)
                    all_positive_parts.append(trigger_words)
                    trigger_display = trigger_words
                
                elif trigger_word_source == "json_sample_prompt":
                    positive_prompt, negative_prompt = self._get_sample_prompt_from_json(lora_path)
                    all_positive_parts.append(positive_prompt)
                    all_negative_parts.append(negative_prompt)
                    trigger_display = positive_prompt
                
                # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ã«è¿½åŠ ï¼ˆæœ«å°¾ã«ã‚«ãƒ³ãƒã‚’ä»˜ã‘ã‚‹ï¼‰
                lora_notation = f"<lora:{lora_name}:{actual_model_str}:{actual_clip_str}>"
                all_text_parts.append(f"{lora_notation}, {trigger_display},")
        
        # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ã‚’çµåˆï¼ˆè¿½åŠ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…ˆé ­ã«é…ç½®ã€æœ«å°¾ã«ã‚«ãƒ³ãƒã‚’ä»˜ã‘ã‚‹ï¼‰
        if additional_prompt.strip():
            # è¿½åŠ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœ«å°¾ã«ã‚«ãƒ³ãƒãŒãªã‘ã‚Œã°è¿½åŠ 
            additional_with_comma = additional_prompt.strip()
            if not additional_with_comma.endswith(','):
                additional_with_comma += ','
            if all_text_parts:
                # LoRAãŒã‚ã‚‹å ´åˆ: è¿½åŠ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + LoRAæƒ…å ±
                positive_text_output = additional_with_comma + "\n" + "\n".join(all_text_parts)
            else:
                # LoRAãŒãªã„å ´åˆ: è¿½åŠ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã¿ï¼ˆæœ«å°¾ã‚«ãƒ³ãƒãªã—ï¼‰
                positive_text_output = additional_prompt.strip()
        else:
            # è¿½åŠ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãªã—
            if all_text_parts:
                # LoRAã®ã¿
                positive_text_output = "\n".join(all_text_parts)
            else:
                # ä½•ã‚‚ãªã„
                positive_text_output = ""
        
        # negativeãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ï¼ˆjson_sample_promptã®å ´åˆã®ã¿å†…å®¹ã‚ã‚Šï¼‰
        negative_text_output = ", ".join([n for n in all_negative_parts if n])
        
        # positiveãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµåˆã—ã¦CONDITIONINGã‚’ç”Ÿæˆï¼ˆè¿½åŠ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å«ã‚ã‚‹ï¼‰
        final_positive_parts = []
        if additional_prompt.strip():
            # LoRAæ§‹æ–‡ã‚’å‰Šé™¤ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã™ã‚‹
            cleaned_prompt = self._remove_lora_syntax(additional_prompt.strip())
            if cleaned_prompt:  # å‰Šé™¤å¾Œã«ç©ºã§ãªã‘ã‚Œã°è¿½åŠ 
                final_positive_parts.append(cleaned_prompt)
        final_positive_parts.extend([p for p in all_positive_parts if p])
        positive_text = ", ".join(final_positive_parts)
        positive_conditioning = self._encode_prompt(
            clip, positive_text, token_normalization, weight_interpretation
        )
        
        # negativeãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµåˆã—ã¦CONDITIONINGã‚’ç”Ÿæˆ
        negative_text = ", ".join([n for n in all_negative_parts if n])
        negative_conditioning = self._encode_prompt(
            clip, negative_text, token_normalization, weight_interpretation
        )
        
        total_loras = len(all_text_parts)
        if total_loras > 0:
            print(f"[RandomLoRALoader] é©ç”¨å®Œäº†: åˆè¨ˆ{total_loras}å€‹ã®LoRA")
        elif additional_prompt.strip():
            print(f"[RandomLoRALoader] é©ç”¨å®Œäº†: LoRAãªã—ã€è¿½åŠ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã¿ä½¿ç”¨")
        else:
            print(f"[RandomLoRALoader] é©ç”¨å®Œäº†: LoRAãªã—ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãªã—")
        
        return (model, clip, positive_text_output, negative_text_output, positive_conditioning, negative_conditioning)


# ComfyUIã¸ã®ãƒãƒ¼ãƒ‰ç™»éŒ²
NODE_CLASS_MAPPINGS = {
    "RandomLoRALoader": RandomLoRALoader
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "RandomLoRALoader": "Random LoRA Loader"
}
